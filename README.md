# üìàCryptoClustering
In this challenge, unsupervised learning was used to predict if cryptocurrencies are affected by 24-hour or 7-day price changes.

This project uses K-Means Clustering and Principal Component Analysis (PCA) to group similar cryptocurrencies based on market behavior and analyze the impact of PCA on clustering results.

Interactive visualizations are created using hvPlot to display:

* Elbow Curves (to find the optimal number of clusters)

* Cryptocurrency clusters in original feature space

* Cryptocurrency clusters in PCA-reduced feature space

### Steps Followed:
#### 1. Data Processing
* Loaded the cryptocurrency market data.
* Scaled features using StandardScaler to normalize values.
#### 2. Clustering with Original Features
* Ran KMeans clustering on the original scaled data.
* Used the Elbow Method to find the optimal number of clusters.
* Visualized clusters based on price_change_percentage_24h and price_change_percentage_7d.
#### 3. Dimensionality Reduction with PCA 
* Applied PCA to reduce the dataset to 3 principal components (PC1, PC2, PC3)
* Verified that PCA preserved approximately 89.5% of the original variance.
#### 4. Clustering with PCA-Reduced Data
* Re-ran KMeans clustering on the PCA-transformed dataset.
* Created a second Elbow Curve for PCA-reduced data.
* Visualized clusters based on PC1 and PC2.
#### 5. Comparative Analysis
* Created composite plots using the + operator to:
    * Compare Elbow Curves (original vs PCA data)
    * Compare cluster scatterplots (original vs PCA data)
### üìù Key Takeaways
##### * Optimal number of clusters:
Both the original data and PCA-reduced data suggested k = 4 as the optimal number of clusters.
##### * Impact of PCA:
PCA successfully reduced the dataset size while preserving most of the important structure.
Clustering on PCA-transformed data produced tighter, more distinct clusters and simplified visualization without significant loss of information.
### Conclusion
Utilizing a reduced number of principal component features in KMeans resulted in clustering that was more stable and less influenced by noise. It did so by reducing the number of features from many original variables, seven to be exact, down to three components (PC1, PC2, and PC3). The clustering captured the main patterns in the data, explaining around 89.5% of the original variance. With the use of fewer features, the clusters also appeared to be more compact and relatively better separated. This indicates that the PCA reduced the noise and redundancy. 

**References**

*Data for this dataset was generated by edX Boot Camps LLC, and is intended for educational purposes only.*